Reload Jane Austin
==================

Intro
-----

> import nltk
> from nltk.book import *
> ' '.join(text2[:150]) # Prints first 150 words of text2
> text2.generate()


Take a step back
----------------

What does the generate() function do?

text.py -> class Text() -> generate()

def generate(self, length=100):
    """
    Print random text, generated using a trigram language model.

    :param length: The length of text to generate (default=100)
    :type length: int
    :seealso: NgramModel
    """
    if '_trigram_model' not in self.__dict__:
        print "Building ngram index..."
        estimator = lambda fdist, bins: LidstoneProbDist(fdist, 0.2)
        self._trigram_model = NgramModel(3, self, estimator=estimator)
    text = self._trigram_model.generate(length)
    print tokenwrap(text)


Do we get it?
* The lambda
* fdist: frequency distribution (look at nltk.probability)
* LidstoneProbDist (nltk.probability)
* trigram model (nltk.model)
* tokenwrap (nltk.util)


Trigram language model
----------------------
* What is a (probabilistic) language model?
Model that computes the probabilities for:
                   ** a sentence to appear P(W)
                   ** an upcoming word of a sentence P(w_n|w_1,w_2,...,w_n-1)

* chain probability rule:
P(x_1,x_2,x_3..,x_n) = P(x_1)P(x_2|x_1)P(x_3|x_1,x_2)...P(x_n|x_1,...,x_n-1)

P("I like strawberry icecream") = P(I)*P(like|I)*P(strawberry|I like)*P(icecream|I like strawberry)

We cannot just count occurancies:
P(sauce|I like strawberry icecream with caramel) = #(I like strawberry icecream with caramel sauce) / #(I like strawberry with caramel)
--> too many possible sentences; we'll never see enough data to estimate these

* Markov assupmtion: simplify!
P(sauce|I like strawberry icecream with caramel) ~~ P(sauce|caramel)
or
P(sauce|I like strawberry icecream with caramel) ~~ P(sauce|with caramel)

* Simplest case: Unigram model:
P(w_1,w_2,...,w_n) ~~ P(w_1) * P(w_2) *...*P(w_n)
in other words
P(sauce|I like strawberry icecream with caramel) ~~ P(sauce)

-> bigram model conditions on the previous word:  P(sauce|caramel)
P(w_i|w_i-1) = count(w_i-1,w_i) / count(w_i-1)

-> trigram model conditions on the sequence of the previous 2 words: P(sauce|with caramel)
Supposedly:
P(w_i|w_i-1,w_i-2) = count(w_i-2,w_i-1,w_i) / count(w_i-1,w_i-2)
-> n-gram models are imperfect modellings of language (language has more complicated long-distance dependencies)
   But they often are good enough for the computations we are interested in (e.g. generate text in the same style as text X)

* So we train that on a training set.

* However, if we use the modal in its "pure" form, no unseen ngrams could be generated! (or predicted)
And the best language model is one that best predicts a (unseen) test set
--> that's what we need the estimator for(and for smoothing) <-- that's called generalization

Generalization (Backoff)
------------------------
P(topping|with caramel) = 0 (doesn't appear in the training set) -> so we have no chance to predict it

1. Generalization method: Add 1/Laplace smoothing (not used for ngram models)
Intuition: when we count occurencies in the training set we add 1 to all the counts;
That way we have a small probability for "others", e.g. for unseen stuff

P_add1 (w_i|w_i-1) = count(w_i-1, w_i) + 1 / count(w_i-1) + V
(V = #|w_i-1 occurs in training set|)

2. Generalization method: Backoff  <-- what NgramModel in nltk uses
Intuition: use less context for unknown stuff
So if we have good evidence we use trigrams, otherwise bigrams, otherwise unigrams (for example)
(We could also mix the three in an interpolation method -> works better)
-> Backoff is simpler (to implement)
TODO: look nearer at Stupid Backoff model + formula

NgramModel
def prob(self, word, context):
    """
    Evaluate the probability of this word in this context using Katz Backoff.

    :param word: the word to get the probability of
    :type word: str
    :param context: the context the word is in
    :type context: list(str)
    """

    context = tuple(context)
    if (context + (word,) in self._ngrams) or (self._n == 1):
        return self[context].prob(word)
    else:
        return self._alpha(context) * self._backoff.prob(word, context[1:])


def generate(self, num_words, context=()):
    '''
    Generate random text based on the language model.

    :param num_words: number of words to generate
    :type num_words: int
    :param context: initial words in generated string
    :type context: list(str)
    '''

    text = list(context)
    for i in range(num_words):
        text.append(self._generate_one(text))
    return text

def _generate_one(self, context):
    context = (self._lpad + tuple(context))[-self._n+1:]
    # print "Context (%d): <%s>" % (self._n, ','.join(context))
    if context in self:
        return self[context].generate()
    elif self._n > 1:
        return self._backoff._generate_one(context[1:])
    else:
        return '.'

Estimator (LidstoneProbDist)
---------------------------
The Lidstone estimate approximates the probability of a sample with count *c* from an
experiment with *N* outcomes and *B* bins as
``c+gamma)/(N+B*gamma)``.  This is equivalant to adding
*gamma* to the count for each bin, and taking the maximum
likelihood estimate of the resulting frequency distribution.

--> TODO: watch about smoothing (add 1 and maximum likelihood)



Some definitions, we would probably need
----------------------------------------
* Token: "A token is the technical name for a sequence of characters—such as hairy , his , or :) —that we want to treat as a group." (NLTK Book)
Example!
* Type: "Number of distinct words; A word type is the form or spelling of the word independently of its specific occurrences in a text—that is, the
word considered as a unique item of vocabulary." (NLTK Book)
Example!
* Lemma: same stem, part of speech, rough word sense
  'cat' and 'cats': same lemma
* Wordform: the full inflected surface form
  'cat' and 'cats': different wordforms


* N-gram: all sequences of n consecutive words in a text
  "We rock linguistics with Python and NLTK."
  bigrams: (we, rock), (rock, linguistics), (linguistics, with), (with, Python), (Python, and), (and, NLTK)
  trigrams: (we, rock, linguistics), (rock, linguistics, with), (linguistics, with, Python), (with, Python, and), (Python, and, NLTK)

References
----------
* NLTK Book
(* Jurafsky + Manning? Coursera class on NLP)
